{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 4), (1, 4), (1, 4), (1, 4), (1, 4), (1, 4))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_all = pd.read_csv('Eve_Metrics/random_forest_all_fea.csv')\n",
    "rf_corr = pd.read_csv('Eve_Metrics/random_forest_corr_fea.csv')\n",
    "xgb_all = pd.read_csv('Eve_Metrics/xgb_all_fea.csv')\n",
    "xgb_corr = pd.read_csv('Eve_Metrics/xgb_corr_fea.csv')\n",
    "gbr_all = pd.read_csv('Eve_Metrics/gbr_all_fea.csv')\n",
    "gbr_corr = pd.read_csv('Eve_Metrics/gbr_corr_fea.csv')\n",
    "\n",
    "rf_all.shape, rf_corr.shape, xgb_all.shape, xgb_corr.shape, gbr_all.shape, gbr_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to normalize each of these scores by transposint the dfs\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By normalizing each evaluation metric, we can compare and assess which one is better since every value is under a uniform scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeEve(df):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    df_t = df.T.reset_index()\n",
    "    df_t.rename(columns={'index': 'metric', 0: 'value'}, inplace=True)\n",
    "    df_t['norm'] = scaler.fit_transform(df_t[['value']])\n",
    "    \n",
    "    df_temp = df_t.T.reset_index()\n",
    "    df_temp.drop('index', axis=1, inplace=True)\n",
    "    df_temp.columns = df_temp.iloc[0]\n",
    "    df_temp.drop(df_temp.index[[0,1]], inplace=True)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2_Score</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  R2_Score RMSE      MAE MAPE\n",
       "2  0.095098  1.0  0.72905  0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_all_norm = normalizeEve(rf_all)\n",
    "rf_all_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_corr_norm = normalizeEve(rf_corr)\n",
    "xgb_all_norm = normalizeEve(xgb_all)\n",
    "xgbcorr_norm = normalizeEve(xgb_corr)\n",
    "gbr_allr_norm = normalizeEve(gbr_all)\n",
    "gbr_corr_norm = normalizeEve(gbr_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateM(rf, xgb, gbr):\n",
    "    cols = rf.columns\n",
    "\n",
    "    for i in cols:\n",
    "        if i == 'R2_Score':\n",
    "            if (rf[i] > xgb[i]).all():\n",
    "                if (rf[i] > gbr[i]).all():\n",
    "                    print('Random Forest model has highest R2 Score')\n",
    "                else:\n",
    "                    print('GBR has the highest R2 Score')\n",
    "            elif (xgb[i] > gbr[i]).all():\n",
    "                print('XGB has the highest R2 Score')\n",
    "            else:\n",
    "                print('GBR has the highest R2 Score')\n",
    "        else:\n",
    "            if (rf[i] < xgb[i]).all():\n",
    "                if (rf[i] < gbr[i]).all():\n",
    "                    print(f'Random Forest model has lowest {i}')\n",
    "                else:\n",
    "                    print(f'GBR has the lowest {i}')\n",
    "            elif (xgb[i] < gbr[i]).all():\n",
    "                print(f'XGB has the lowest {i}')\n",
    "            else:\n",
    "                print(f'GBR has the lowest {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB has the highest R2 Score\n",
      "XGB has the lowest RMSE\n",
      "Random Forest model has lowest MAE\n",
      "GBR has the lowest MAPE\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the all features of each model\n",
    "evaluateM(rf_all_norm, xgb_all_norm, gbr_allr_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBR has the highest R2 Score\n",
      "GBR has the lowest RMSE\n",
      "GBR has the lowest MAE\n",
      "GBR has the lowest MAPE\n"
     ]
    }
   ],
   "source": [
    "evaluateM(rf_corr_norm, xgbcorr_norm, gbr_corr_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that gradient boost model is better when only selected with the positively correlating features\n",
    "\n",
    "On the other hand when all features are taken into consideration, XG Boost shows the best performance compared to the other 2 models.\n",
    "\n",
    "We can choose gradient boost model as it has better results in both scenarios compared to other 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
